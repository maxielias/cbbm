{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "import benchmarks\n",
    "import transform_functions\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler, MaxAbsScaler, Normalizer, StandardScaler, PowerTransformer, QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_features = ['consumer_loans_assets', 'dep_cap_assets', 'loans_cap_assets', 'business_loans_assets', 'asset_tot_asset']\n",
    "with open(\"data/kpi.json\", \"r\") as kpi_json:\n",
    "    kpi_list = json.load(kpi_json)\n",
    "data_kpis = pd.read_csv(\"data/kpis.csv\")\n",
    "data_kpis = data_kpis[data_kpis[\"ind\"].isin(key_features)]\n",
    "kpis = data_kpis[data_kpis[\"ind\"].isin(key_features)][\"ind\"].unique()\n",
    "#print(kpis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscwd = os.getcwd()\n",
    "\n",
    "# Load dataframe with average weighted value of loan segments\n",
    "df_entidad = pd.read_csv(os.path.join(oscwd, \"data/ENTIDAD.txt\"), encoding=\"utf-8\",\n",
    "                            dtype = {\n",
    "                                    'EENTIDAD': int, 'CODNUM': int, \n",
    "                                    'NOMBRE CORTO': str, \"VIGENTE\": int, \n",
    "                                    \"FECHA_ALTA\": str, \"BAJA\": str\n",
    "                                    }\n",
    "                                    # parse_dates=['Periodo'],\n",
    "                            )\n",
    "df_entidad = df_entidad[[\"EENTIDAD\", \"CODNUM\", \"NOMBRE CORTO\", \"VIGENTE\", \"FECHA_ALTA\", \"BAJA\"]]\n",
    "# Load dataframe with operative banks\n",
    "active = pd.DataFrame(df_entidad[\"EENTIDAD\"][df_entidad[\"VIGENTE\"]==1])\n",
    "active.columns = [\"bank\"]\n",
    "# Create list of operative banks that are actually in the balance sheet statement\n",
    "# banks = [a for a in active[\"bank\"] if a in df_evoldatos[\"bank\"].unique()]\n",
    "#print(active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_operative = pd.read_csv(\"data/not_predicted.csv\", header=0)\n",
    "#not_operative.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_gde = [11, 14, 29, 300] # list(df_entidad[\"CODIGO\"][df_entidad[\"Grupo\"]==8.0])\n",
    "digit = [143, 158, 384, 45030]\n",
    "auto = [340, 44059, 44088, 44092, 44093, 44094, 44095, 44096, 44098, 44099]\n",
    "drop_ent = pub_gde + digit + auto\n",
    "#print(drop_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Silhouette Method-KMeans-[4, 4]-MaxAbs Scaler-Isolation Forest.csv', 'Silhouette Method-KMeans-[4, 4]-MaxAbs Scaler-Local Outlier Factor.csv', 'Silhouette Method-KMeans-[4, 4]-MaxAbs Scaler-Robust Covariance.csv', 'Silhouette Method-KMeans-[4, 4]-Power Transformer-Isolation Forest.csv', 'Silhouette Method-KMeans-[4, 4]-Power Transformer-One-Class SVM (SGD).csv', 'Silhouette Method-KMeans-[4, 4]-Power Transformer-One-Class SVM.csv', 'Silhouette Method-KMeans-[4, 4]-Power Transformer-Robust Covariance.csv', 'Silhouette Method-KMeans-[4, 4]-Robust Scaler-Local Outlier Factor.csv', 'Silhouette Method-KMeans-[4, 4]-Standard Scaler-One-Class SVM (SGD).csv', 'Silhouette Method-KMeans-[4, 4]-Standard Scaler-One-Class SVM.csv', 'Silhouette Method-KMedoids-[4, 4]-Power Transformer-Isolation Forest.csv', 'Silhouette Method-KMedoids-[4, 4]-Power Transformer-One-Class SVM (SGD).csv', 'Silhouette Method-KMedoids-[4, 4]-Power Transformer-Robust Covariance.csv', 'Silhouette Method-MiniBatchKMeans-[4, 4]-Normalizer-Robust Covariance.csv', 'Silhouette Method-MiniBatchKMeans-[4, 4]-Power Transformer-Isolation Forest.csv', 'Silhouette Method-MiniBatchKMeans-[4, 4]-Power Transformer-Local Outlier Factor.csv', 'Silhouette Method-MiniBatchKMeans-[4, 4]-Standard Scaler-One-Class SVM (SGD).csv']\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(\"final/gh_2008_2018_v2\") if isfile(join(\"final/gh_2008_2018_v2\", f))]\n",
    "onlyfiles = [f for f in onlyfiles if f.startswith(\"Silhouette Method\") and f.endswith(\".csv\")]\n",
    "print(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following columns were dropped: []\n",
      "KMeans(algorithm='elkan', max_iter=1000, n_clusters=4)\n",
      "    bank  method0\n",
      "0     72        3\n",
      "1    339        0\n",
      "2  65203        1\n",
      "The following columns were dropped: []\n",
      "KMeans(algorithm='elkan', max_iter=1000, n_clusters=4)\n",
      "     bank  method1\n",
      "0      45        0\n",
      "1      72        0\n",
      "2     165        1\n",
      "3     191        0\n",
      "4     198        0\n",
      "5     259        0\n",
      "6     285        0\n",
      "7     331        3\n",
      "8     332        3\n",
      "9     339        1\n",
      "10  44077        3\n",
      "11  44090        3\n",
      "12  65203        3\n",
      "The following columns were dropped: []\n",
      "KMeans(algorithm='elkan', max_iter=1000, n_clusters=4)\n",
      "    bank  method2\n",
      "0      7        0\n",
      "1     17        0\n",
      "2     72        0\n",
      "3    332        3\n",
      "4    339        1\n",
      "5  65203        3\n",
      "The following columns were dropped: []\n",
      "KMeans(algorithm='elkan', max_iter=1000, n_clusters=4)\n",
      "   bank  method3\n",
      "0   165        0\n",
      "1   198        0\n",
      "2   332        0\n",
      "3   339        3\n",
      "The following columns were dropped: []\n",
      "KMeans(algorithm='elkan', max_iter=1000, n_clusters=4)\n",
      "    bank  method4\n",
      "0      7        3\n",
      "1     44        3\n",
      "2    191        3\n",
      "3    312        3\n",
      "4    332        3\n",
      "5    339        0\n",
      "6  65203        3\n",
      "The following columns were dropped: []\n",
      "KMeans(algorithm='elkan', max_iter=1000, n_clusters=4)\n",
      "    bank  method5\n",
      "0    331        3\n",
      "1    332        3\n",
      "2    339        3\n",
      "3  44077        0\n",
      "4  44090        0\n",
      "5  65203        0\n",
      "The following columns were dropped: []\n",
      "KMeans(algorithm='elkan', max_iter=1000, n_clusters=4)\n",
      "    bank  method6\n",
      "0    305        3\n",
      "1    332        3\n",
      "2    339        1\n",
      "3  44090        3\n",
      "4  65203        3\n",
      "The following columns were dropped: []\n",
      "KMeans(algorithm='elkan', max_iter=1000, n_clusters=4)\n",
      "     bank  method7\n",
      "0      45        1\n",
      "1      72        1\n",
      "2     165        1\n",
      "3     191        1\n",
      "4     198        1\n",
      "5     259        1\n",
      "6     285        1\n",
      "7     331        3\n",
      "8     332        3\n",
      "9     339        0\n",
      "10  44077        3\n",
      "11  44090        3\n",
      "12  65203        3\n",
      "The following columns were dropped: []\n",
      "KMeans(algorithm='elkan', max_iter=1000, n_clusters=4)\n",
      "   bank  method8\n",
      "0    72        3\n",
      "1   259        3\n",
      "2   305        3\n",
      "3   332        3\n",
      "4   339        0\n",
      "The following columns were dropped: []\n",
      "KMeans(algorithm='elkan', max_iter=1000, n_clusters=4)\n",
      "    bank  method9\n",
      "0    310        0\n",
      "1    331        0\n",
      "2    332        0\n",
      "3  44077        0\n",
      "4  44090        0\n",
      "5  65203        0\n",
      "The following columns were dropped: []\n",
      "KMedoids(init='k-medoids++', max_iter=1000, method='pam', n_clusters=4)\n",
      "   bank  method10\n",
      "0   165         1\n",
      "1   198         1\n",
      "2   332         1\n",
      "3   339         1\n",
      "The following columns were dropped: []\n",
      "KMedoids(init='k-medoids++', max_iter=1000, method='pam', n_clusters=4)\n",
      "    bank  method11\n",
      "0      7         1\n",
      "1     44         1\n",
      "2    191         1\n",
      "3    312         1\n",
      "4    332         1\n",
      "5    339         1\n",
      "6  65203         1\n",
      "The following columns were dropped: []\n",
      "KMedoids(init='k-medoids++', max_iter=1000, method='pam', n_clusters=4)\n",
      "    bank  method12\n",
      "0    305         3\n",
      "1    332         3\n",
      "2    339         3\n",
      "3  44090         3\n",
      "4  65203         3\n",
      "The following columns were dropped: []\n",
      "MiniBatchKMeans(init='random', max_iter=10000, n_clusters=4, n_init=10)\n",
      "    bank  method13\n",
      "0      7         0\n",
      "1     72         0\n",
      "2    312         1\n",
      "3    339         2\n",
      "4  65203         3\n",
      "The following columns were dropped: []\n",
      "MiniBatchKMeans(init='random', max_iter=10000, n_clusters=4, n_init=10)\n",
      "   bank  method14\n",
      "0   165         3\n",
      "1   198         3\n",
      "2   332         2\n",
      "3   339         2\n",
      "The following columns were dropped: []\n",
      "MiniBatchKMeans(init='random', max_iter=10000, n_clusters=4, n_init=10)\n",
      "     bank  method15\n",
      "0      44         1\n",
      "1      45         1\n",
      "2     165         2\n",
      "3     191         1\n",
      "4     198         1\n",
      "5     259         2\n",
      "6     266         1\n",
      "7     305         1\n",
      "8     310         1\n",
      "9     312         1\n",
      "10    331         3\n",
      "11    332         3\n",
      "12    339         2\n",
      "13  44077         2\n",
      "14  44090         3\n",
      "15  65203         3\n",
      "The following columns were dropped: []\n",
      "MiniBatchKMeans(init='random', max_iter=10000, n_clusters=4, n_init=10)\n",
      "   bank  method16\n",
      "0    72         1\n",
      "1   259         1\n",
      "2   305         1\n",
      "3   332         1\n",
      "4   339         3\n",
      "     bank  method0  method1  method2  method3  method4  method5  method6  \\\n",
      "0       7      0.0      2.0      0.0      3.0      3.0      1.0      1.0   \n",
      "1      11      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "2      14      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "3      15      0.0      2.0      1.0      3.0      0.0      1.0      1.0   \n",
      "4      16      0.0      2.0      1.0      3.0      0.0      1.0      1.0   \n",
      "..    ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "73  44099      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "74  45030      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "75  45056      3.0      3.0      0.0      0.0      3.0      0.0      3.0   \n",
      "76  45072      0.0      1.0      1.0      2.0      1.0      3.0      2.0   \n",
      "77  65203      1.0      3.0      3.0      1.0      3.0      0.0      3.0   \n",
      "\n",
      "    method7  method8  method9  method10  method11  method12  method13  \\\n",
      "0       0.0      0.0      3.0       0.0       1.0       2.0       0.0   \n",
      "1       NaN      NaN      NaN       NaN       NaN       NaN       NaN   \n",
      "2       NaN      NaN      NaN       NaN       NaN       NaN       NaN   \n",
      "3       0.0      0.0      3.0       0.0       3.0       2.0       0.0   \n",
      "4       0.0      0.0      3.0       0.0       3.0       2.0       2.0   \n",
      "..      ...      ...      ...       ...       ...       ...       ...   \n",
      "73      NaN      NaN      NaN       NaN       NaN       NaN       NaN   \n",
      "74      NaN      NaN      NaN       NaN       NaN       NaN       NaN   \n",
      "75      1.0      NaN      0.0       1.0       1.0       3.0       0.0   \n",
      "76      0.0      0.0      1.0       2.0       2.0       0.0       2.0   \n",
      "77      3.0      2.0      0.0       3.0       1.0       3.0       3.0   \n",
      "\n",
      "    method14  method15  method16  \n",
      "0        2.0       2.0       2.0  \n",
      "1        NaN       NaN       NaN  \n",
      "2        NaN       NaN       NaN  \n",
      "3        2.0       2.0       2.0  \n",
      "4        2.0       2.0       2.0  \n",
      "..       ...       ...       ...  \n",
      "73       NaN       NaN       NaN  \n",
      "74       NaN       NaN       NaN  \n",
      "75       3.0       NaN       NaN  \n",
      "76       0.0       0.0       3.0  \n",
      "77       1.0       3.0       0.0  \n",
      "\n",
      "[78 rows x 18 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_groups_df = pd.DataFrame({\"bank\":active[\"bank\"]})\n",
    "for n, o in enumerate(onlyfiles):\n",
    "    path_gh = f\"final/gh_2008_2018_v2/{o}\"\n",
    "    split1 = path_gh.split(\"[\")[0]\n",
    "    split2 = path_gh.split(\"]\")[-1]\n",
    "    path_model = f\"{split1}[]{split2}\".split(\".\")[0]+\".pkl\"\n",
    "    # print(o)\n",
    "    #print(path_model)\n",
    "    groups = pd.read_csv(path_gh)\n",
    "    groups.columns = [\"bank\", \"method\"+str(n)]\n",
    "    bank_list = [b for b in groups[\"bank\"]]\n",
    "    outliers = [a for a in active[\"bank\"] if a not in bank_list]\n",
    "    outliers = [a for a in outliers if a not in list(not_operative[\"bank\"])]\n",
    "    outliers = [a for a in outliers if a not in drop_ent]\n",
    "    start = 2008\n",
    "    end = 2018\n",
    "    data = transform_functions.get_mean_pivot_table(data_kpis, 0, 2, 1, 3, start, end)\n",
    "    df_corr, data = benchmarks.find_and_drop_correlated_features(data, labels_col=\"ent\", threshold=0.7, drop=True, plot=False)\n",
    "    data = data[data[\"ent\"].isin(outliers)]\n",
    "    data_arr = data.iloc[:,1:].to_numpy()\n",
    "    clustering_model = joblib.load(path_model)\n",
    "    print(clustering_model)\n",
    "    predicted_outliers = clustering_model.predict(data_arr)\n",
    "    outliers_df = pd.DataFrame({\"bank\":outliers, \"method\"+str(n):predicted_outliers})\n",
    "    print(outliers_df)\n",
    "    group_df = pd.concat([groups, outliers_df])\n",
    "\n",
    "    all_groups_df = pd.merge(all_groups_df, group_df, how=\"left\", on=\"bank\")\n",
    "\n",
    "print(all_groups_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_groups_df.to_csv(\"final\\gh_2008_2018_v2\\ghs_inliers_outliers_predicted.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"groups = pd.read_csv(path_gh)\n",
    "bank_list = [b for b in groups[\"sample\"]]\"\"\"\n",
    "#bank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"outliers = [a for a in active[\"bank\"] if a not in bank_list]\n",
    "outliers = [a for a in outliers if a not in list(not_operative[\"bank\"])]\n",
    "outliers = [a for a in outliers if a not in drop_ent]\"\"\"\n",
    "#outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# data_kpis = pd.read_csv(\"data/kpis.csv\")\n",
    "start = 2008\n",
    "end = 2018\n",
    "data = transform_functions.get_mean_pivot_table(data_kpis, 0, 2, 1, 3, start, end)\n",
    "df_corr, data = benchmarks.find_and_drop_correlated_features(data, labels_col=\"ent\", threshold=0.7, drop=True, plot=False)\n",
    "data = data[data[\"ent\"].isin(outliers)]\n",
    "data_arr = data.iloc[:,1:].to_numpy()\"\"\"\n",
    "#data_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"clustering_model = joblib.load(path_model)\"\"\"\n",
    "#clustering_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"predicted_outliers = clustering_model.predict(data_arr)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#print(outliers)\n",
    "#print(predicted_outliers)\n",
    "outliers_df = pd.DataFrame({\"sample\":outliers, \"Silhouette Method\":predicted_outliers})\n",
    "#print(groups)\n",
    "group_df = pd.concat([groups, outliers_df])\n",
    "print(len(group_df))\n",
    "display(group_df)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbbm-RQGQLPr7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bd4a477d0876910250915e3e9f7d6b6cb97ff716c94d5f9b489478001bf4dca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
